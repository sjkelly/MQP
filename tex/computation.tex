\chapter{Computational Definitions and Grammar}

Programming languages are the grammar and syntax a computer presents to a user.
This project is fundamentally exploratory in nature and seeks to generate
understanding of geometric relationships using the intersection of
mathematical and computational rigor. We have chosen to use the Julia
programming language due to comfort of development, and an abundance of
supporting libraries for mathematical computation. In this chapter
we will give a brief introduction to many computing concepts and illustrate
how Julia advances them to meet our needs well.

\section{History}
Julia is a programming language first released in early 2012 by a group of
developers from MIT. The language targets technical computing by providing a
dynamic type system with near-native code performance. This is accomplished by
using three concepts: a Just-In-Time (JIT) compiler to target the LLVM framework,
a multiple dispatch system, and code specialization\cite{bezanson2012julia}
\cite{Bezanson_Edelman_Karpinski_Shah_2014}.
More simply, the language is designed to be dynamic in a way that allows
rapid prototyping of code and understandable to a reader, yet provides
a design amicable to performance optimizations and specialization.
Dynamic type systems allow the programmer to ignore or selectively
specify type information, such
as the bytes in an integer, and alow the compiler to infer this information
based on the input types. JIT compilation means code is compiled during runtime
which allows functions to be recompiled and thus optimized for varous
data types.
The syntactical style is similar to MATLAB and Python.
The language implementation and many libraries are available under the
permissive MIT license.\footnote{\url{http://opensource.org/licenses/MIT}}

Benchmarks have shown the language can consistently perform within a factor of
two of native C and FORTRAN code.\footnote{\url{http://julialang.org/benchmarks}}
This is enticing for a solid modeling application and for numerical analysis,
as the code abstraction can grow organically without performance penalty.
In fact, the authors of Julia call this balance a solution to the 
``two language problem". The problem is encountered when abstraction in a
high-level language will disproportionately affect performance unless
implemented in a low-level language. In the next sections we will compare
the expressibility and performance to other languages.

\section{Comparisons}

Many languages are as fast as Julia but sacrifice expressibility.
In Figure \ref{fig:juliabench} we can see some comparisons to other programming
languages. This was developed by the Julia core team, and illustrates that
Julia is highly competitive in performance. Again, these results stem from
the compiler and language design. In Figure \ref{fig:juliaexpr} we can see
these results normalized against code length. The Julia code is quite short,
yet consistently achieves good performance.
Thus the programmer may write less code and spend less time waiting
for results in an interactive environment, which makes Julia a great choice
for exploratory programming.
Much of this comes down to the innovated type and function system.\cite{Chen2014} We will
discuss these more in depth later.

\begin{figure}[h!]
  \centering
    \includegraphics[width=1.0\textwidth]{img/juliabench.pdf}
  \caption{A comparison of programming languages and performance.}
  \label{fig:juliabench}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=1.0\textwidth]{img/expressability.pdf}
  \caption{The results in Figure \ref{fig:juliabench} normalized for code length. (Courtesy of Simon Danish)}
  \label{fig:juliaexpr}
\end{figure}


In 1972 Alan Kay introduced the terms
``class" and ``object" to describe a coupling of data and functionality.\footnote{\url{http://gagne.homedns.org/~tgagne/contrib/EarlyHistoryST.html}}
An object is an instance of a class, which contains the definitions of 
functions and member data. Computer Scientists
call this "Object Oriented Programming" (OOP).
Languages such as C++, Java, and Python all subscribe to this paradigm.
In Python this looks like the following:
\begin{lstlisting}
class Foo:
    foo1
    foo2
    def add_to_foo1(self, x):
        self.foo1 += x
\end{lstlisting}

This system positively enables specialization of functionality, but due
to the coupling of data with functions it becomes a challenge to extend
functionality. Languages for scientific computing generally avoid the
``traditional" notions
of OOP, preferring rather to seperate data from
functionality. In Table \ref{tab:types} we can see a
comparison of type systems used in scientific computing languages. Here ``Type
system" can be either dynamic or static, where in a static system the programmer
needs to specify to the program compiler how data is transformed in a function.
Generic functions allow a single function name, for example \texttt{sum}, to have
multiple defintions with execution contigent upon the matching of argument
types. Many programmers may first encounter generic functions through the
term ``overloaded" function as well.
The definition of a parametric type is more nuanced, but generally
means that the definition of a type may vary based on the types of it's
member data. We will dedicate a section to the explaination of type parameters.
In the next
few sections these ideas will hopefully be clarified and the implications of
multiple dispatch and the relation to OOP will be developed further.


\begin{figure}[h!]
  \centering
    \caption{A comparison of functions, typing, and dispatch.}
    \begin{tabular}{ l | l l l}
    Language & Type system & Generic functions & Parametric types \\
    \hline
    Julia & dynamic & default & yes \\
    Common Lisp & dynamic & opt-in & yes (but no dispatch) \\
    Dylan & dynamic & default & partial (no dispatch) \\
    Fortress & static & default & yes \\
    \end{tabular}
  \label{tab:types}
\end{figure}


\section{Functions}
Julia is an experiment in language design. Much of the advancement
revolves around the representation of data and the execution of functions.
The language is optionally or dynamically typed, which means function specialization on types
is inferred by the compiler without user intervention. This is an idea
first utilized in the Hadley Milner's ``ML" which was created to develop theorem
provers\cite{Harper_1997}. The compiler analizes program flow and is able
to infer the types of variables and function returns.
A basic example of inference in Julia is shown below:
\begin{lstlisting}
julia> increment(x) = x + 1
increment (generic function with 1 method)

julia> increment(1)
2

julia> increment(1.0)
2.0
\end{lstlisting}\footnote{The REPL (Read-Eval-Print-Loop) allows interactive
evaluation of Julia code. It is highly useful for exploration and testing of
ideas in the language.
Blocks starting with "\texttt{julia>}" represent input and the preceding
line represents output of the evaluated line.
}
The \texttt{increment} function was defined for any \texttt{x} value. When the
\texttt{1}, an
integer type was passed as an argument, an integer was returned. Likewise
when a floating point, \texttt{1.0} was passed, the floating point
\texttt{2.0} was returned.

Let's see what happens when we try a string:
\begin{lstlisting}
julia> increment("a")
ERROR: MethodError: `+` has no method matching +(::ASCIIString, ::Int64)
Closest candidates are:
  +(::Any, ::Any, ::Any, ::Any...)
  +(::Int64, ::Int64)
  +(::Complex{Bool}, ::Real)
  ...
 in increment at none:1
\end{lstlisting}

The problem is that the \texttt{+} function is not implemented between the
\texttt{ASCIIString} and \texttt{Int64} types.
We need to either implement a \texttt{+} function
which might be ambiguous, or specialize the function for \texttt{ASCIIString}.
A specific implementation is preferrable in this case:
\begin{lstlisting}
julia> function increment(x::ASCIIString)
           ASCIIString([increment(c) for c in x])
       end
increment (generic function with 2 methods)
\end{lstlisting}
The line \texttt{x::ASCIIString} is called a ``type annotation" and
states that \texttt{x} must be a subtype
of \texttt{ASCIIString}. This allows one to control dispatch of types to
functions,
since Julia will default to the \emph{most specific implementation}
for the type.
Since\texttt{ASCIIString} is a series of 8 bit characters, we can iterate over the
string and increment each character individually. The \texttt{[]} indicates we are
constructing an array of characters to pass to be passed to the \texttt{ASCIIString}
type constructor. Now we see our example works:
\begin{lstlisting}
julia> increment("abc")
"bcd"
\end{lstlisting}

What was demonstrated here is the concepts of specialization and multiple
dispatch, both are highly coupled topics.
Each function call in Julia is specialized for types if possible.
This means the author only has to write a few sufficently abstract
implementations of functions. If special cases occur, multiple functions
with different arity or type signatures can be implmented. Explicitly
this is called multiple dispatch. In practice by the user this looks like
abstracted or generic code if done well so many types can be handled
by one function.
To the computer, this means choosing or generating the most specific and
performant method\footnote{Functions and methods are distinct in Julia. A
function may be thought of in the mathematical sense. A method is a
function specialized on types with unique machine code.}
Let's go back to the integer and floating point
example. Below is the LLVM assembly generated for each method:
\begin{lstlisting}
julia> @code_llvm increment(1)

define i64 @julia_increment_21458(i64) { // <return type> <function name>(<arg type>)
top:
  %1 = add i64 %0, 1
  ret i64 %1 // return <return type> <return id>
}

julia> @code_llvm increment(1.0)

define double @julia_increment_21466(double) {
top:
  %1 = fadd double %0, 1.000000e+00
  ret double %1
}
\end{lstlisting}

Note we have annotated the LLVM code so this is understandable. 
The only real similarity is the line count. Each one of these functions are generated by the
Julia compiler at run time.

Many of the concepts used for performance also serve as methods for
expressability. In this case, multiple dispatch used by the compiler for
specialization of functions reveals itself as a way for the user to
specialize over many types. In summary, the
basic steps in generating native computer code from a function are to:
\begin{enumerate}
\item Parse the expression
\item Infer type information
\item Generate native machine code and optimizations
\end{enumerate}

\section{Types}

\subsection{Mutability and Data Packing}
Types and immutables are containers of data. The primary difference between
the two is the notion of ``mutability". Types are mutabile, immutables are 
immutable. What does this mean? Let's break something first via the REPL:
\begin{lstlisting}
julia> type FooIsMutable
           a
       end

julia> f = FooIsMutable(1)
FooIsMutable(1)

julia> f.a
1

julia> f.a = 2
2

julia> f.a
2

julia> immutable FooIsImmutable
           a
       end

julia> f = FooIsImmutable(1)
FooIsImmutable(1)

julia> f.a
1

julia> f.a = 2
ERROR: type FooIsImmutable is immutable
\end{lstlisting}

What just happened demonstrates the contract defined by mutability. Mutable
objects, which is an instance of a type (i.e. \texttt{f}), can have their fields
(i.e. \texttt{a}) changed. Immutables cannot. The immutable contract helps develop
a notion of functional purity. To the user this means immutables are defined
by their values. This can be of great benefit to avoid errors and establish
concrete equality between types, such as vectors.
Practically this can be of great benefit to
the compiler to determine invariants and eliminate pointers in a datatype.
For example:
\begin{lstlisting}
julia> a = (1,2,3)
(1,2,3)

julia> b = typeof(a)
Tuple{Int64,Int64,Int64}

julia> isbits(b)
true

julia> a = ([1],[2],[3])
([1],[2],[3])

julia> b = typeof(a)
Tuple{Array{Int64,1},Array{Int64,1},Array{Int64,1}}

julia> isbits(b)
false
\end{lstlisting}

\texttt{isbits} ask the question ``will this type be tightly packed in memory
without pointers to values"? A
\texttt{Tuple} is a fixed-length set of linear, ordered, data. It has syntax for
construction with \texttt{()}. In computations we want our data be close together
for fast access. In modern times we call such data ``cache friendly", or
``cache localized", which means the computer may store the data in
registers closer to the CPU.
Immutability helps us achieve this. Let's look that the
types inside the 3-tuples and see their \texttt{isbits} status:
\begin{lstlisting}
julia> isbits(Array{Int64,1})
false

julia> isbits(Int64)
true
\end{lstlisting}
Why is this the case? We see that \texttt{Int64} is bits, because it is literally
64 bits. In Julia a \texttt{bitstype} behaves similar to an immutable, and is identified
by value. For example the defintion for \texttt{Int64} is \texttt{bitstype 8 Int64}
which means an \texttt{Int64} is 8 bytes long.
\texttt{Array\{Int64,1\}} is a mutable data type that can vary in size.
This means
the \texttt{Tuple} needs to store the arrays as references to memory,
in this case a
pointer. When iterating over a data set, such a ``pointer dereferences" (this is
jargon for accessing the data in memory pointed to by a pointer), can be costly.
Modern CPUs excel when data is linearly packed and pointer-free. The
data can be brought into the CPU's memory cache and registers only once
and computed without
shuffling between cache and RAM. The cost of lookup time between the cache and
RAM generally differs by several orders of magnitude.

\subsection{Parameters}



\section{Macros and Generated Functions}
Julia is a descendant of the Lisp family of programming languages. Lisp
is a portmanteau for ``List Processing". The language was designed to address
the new notion of ``types", specifically in application to Artificial
Intelligence (AI) problems\cite{McCarthy_1966}. The notion of an ``S-Expression"
was introduced in McCarthy's seminal work, ``Recursive functions of symbolic expressions and their
computation by machine". These statements use parenthesis
to denote functions and arguments. Below is an an example of S-Expressions
for addition and multiplication.

\begin{lstlisting}
> (+ 1 1)
2

> (* 3 4)
12
\end{lstlisting}

This syntax is noted for it's mathematical purity.
However it can be a syntactic difficulty for many.
Most of the current popular programming languages
use variants of ALGOL syntax, which is noted for being more readable
\cite{Hoare}.
Julia also uses ALGOL syntax, but is converted to S-Expressions after parsing
\cite{julia_internals}.
This enables
many of the mathematically pure relations we seek to achieve.
In addition S-Expressions are highly conducive to source transforms.
This develops a notion of ``Homoiconicity", where the representation of
program structure is similar to the syntax. In Julia we use this property
to make ``macros" which enable source code to be transformed based on
the syntactical structure before compilation.

Generated functions perform a similar function as macros, but at the function
level. They enable source code to be procedurally generated based on types.
This allows the user fine-tuned control of the compilation process, and
will allow optimizations to be performed that are not currently
available in the compiler.
Surveys of Computer Science literature show that such a concept is new in
a programming language that uses type inference
\cite{julia_metaprog}.
However the use of generated functions is generally frowned upon by the
Julia community since it makes compilation more difficult since type inference
has to be run multiple times before compilation may happen.
This often slows down trivial functions by
several orders of magnitude, and should be only used if a method is called
many times and performancde is critical.

We will omit an introduction of macros and generated functions
as they are advanced langauge
features. For our purposes a basic understanding of the terminology will
be sufficient.


\section{Numerical Robustness}

Numerical robustness is a perennial problem in computational geometry\cite{Shamos_1999}.
Multiple
approaches exists for various numeric types. Floating points are by far
the most difficult to deal with. Tools such as Gappa have been developed so
algorithm writers can check their invariants when using floating points\cite{Gappa}.
Such tools complicate software development and are not an accessible option
for the casual researcher.

One of the most common problems formulated is to determine whether or not a
point is collinear with a line segment. Shewchuk has one of the most pragmatic
and robust treatments on this topic\cite{Shewchuk}. Kettner, et. al. have also
developed more examples where numerical robustness is critical\cite{Kettner_Mehlhorn_Pion_Schirra_Yap_2008}.

Julia's GeometricalPredicates package \footnote{\url{https://github.com/JuliaGeometry/GeometricalPredicates.jl}}
uses the approach outlined by Volker Springel, which requires all floating point
numbers to be scales between 1 and 2\cite{Springel_2010}. This has the downside
of significantly reducing the available resolution to 50\% of the available
floating point numbers.

A simpler, although less applicable, approach is to work
within integer space. Developing a system around this is of interest. For
example, it should be possible to specify a minimum unit (e.g. microns)
and perform all computations in integer space assuming this does not exceed
the needed resolution. More importantly, modern CPUs have integrated 128 bit
Integer support. 170141183460469231731687303715884105727 is a lot of microns.



